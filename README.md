# Airflow 101

## Project

#### Challenge

Нужно узнавать просмотры для каждого элемента в списке.
1. Элементы добавляются. Вот ссылка: (ссылка в TG)
1. Нужно распарсить каждую из ссылок, вытащить из неё количество просмотром и записать обратно в эксельку.
1. Это должен делать Airflow DAG, который выполняется каждую ночь. Каждое выполнение он проверяет, парсил ли он уже каждую ссылку за последние два дня. Это нужно, чтобы не парсить одну и ту же ссылку несколько раз.
1. После своего выполнения он должен писать репорт в телеграм-чат: сколько ссылок обработаны удачно, у скольких ошибка и у каких именно ошибка (номера строк в документе и сама ссылка).
1. Ещё нужен мониторинг: пусть DAG пишет в Prometeus стандартные метрики для дага.
1. Для мониторинга бизнес-метрик вы можете использовать поднятый для вас statsd и экспортер
---
 
#### Решение
1. Датасет из google-таблицы скачивается локально.
   Результат парсинга тоже хранится локально - для проверки времени прошлой обработки.
1. Для части сайтов получалось много ложно-негативного результата при попытке парсить BeautifulSoup-ом.
   Пришлось подключить Selenium.
1. Отрепетировали переезд на новый, более мощный, сервер. Т.к. один из наших серверов под грузом
   Selenium парсил часов 12, а второй - в GoogleCloud, откуда не доступен Vimeo.
1. Чтобы ускорить процесс, придумали разбивать датасет на части и парсить сгенерированными параллельными
   тасками.
   
   ![DAG schema](http://i.piccy.info/i9/d3a2fc052d0981b8fbdd5151a71649c7/1592991993/23240/1380201/74489dag_tasks.png)

   Файл с ДАГом [air101_project_with_parts](https://github.com/dimk00z/airflow_project/blob/master/air_project_parallels_dag.py) содержит сделующие таски:

  - `load_links_from_gsheet` - загружает данные стобца из заданной таблицы и сохраняет их в `cvs` файл.
  - `parse_links_watchers_list` - список тасок, которые параллельно парсят часть полученных ссылок. Количество параллельных задач задается переменной `PARTS_NUMBER`( по умолчанию 4). При проверке учитываются предыдущие выполнения, пересканируются только ссылки которые проверялись свыше двух суток назад.
  - `write_to_gsheet` - пишет результат парсинга в гугл-таблицу.
  - `send_report` - шлет репорт в телеграм-чат и в `statsd`: сколько ссылок обработаны удачно, у скольких ошибка и у каких именно ошибка (список ошибок прикрепляется файлом)

##### Мониторинг 

- `Grafana` получает как стандартные метрики с exportr-а, так и метрики от `statsd`

![Grafana screenshot](https://i.piccy.info/i9/173ed420df876b4274b896aabcd2bd0f/1592989338/61121/1380201/75095airflow101_project_grafana_800.jpg)
___

#### Цель проекта

Код написан в образовательных целях на онлайн-курсе [Airflow 101](https://airflow101.python-jitsu.club/).
